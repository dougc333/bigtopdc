# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

Source: hadoop
Section: misc
Priority: extra
Maintainer: Bigtop <bigtop-dev@incubator.apache.org>
Build-Depends: debhelper (>= 6), ant, ant-optional, liblzo2-dev, python, libzip-dev,automake, autoconf (>= 2.61), sharutils, g++ (>= 4), git-core, libfuse-dev, libssl-dev
Standards-Version: 3.8.0
Homepage: http://hadoop.apache.org/core/

Package: hadoop
Provides: hadoop
Architecture: all
Depends: ${shlibs:Depends}, ${misc:Depends}, adduser, bigtop-utils
Recommends: hadoop-native
Description: A software platform for processing vast amounts of data
 Hadoop is a software platform that lets one easily write and
 run applications that process vast amounts of data.
 .
 Here's what makes Hadoop especially useful:
  * Scalable: Hadoop can reliably store and process petabytes.
  * Economical: It distributes the data and processing across clusters
                of commonly available computers. These clusters can number
                into the thousands of nodes.
  * Efficient: By distributing the data, Hadoop can process it in parallel
               on the nodes where the data is located. This makes it
               extremely rapid.
  * Reliable: Hadoop automatically maintains multiple copies of data and
              automatically redeploys computing tasks based on failures.
 .
 Hadoop implements MapReduce, using the Hadoop Distributed File System (HDFS).
 MapReduce divides applications into many small blocks of work. HDFS creates
 multiple replicas of data blocks for reliability, placing them on compute
 nodes around the cluster. MapReduce can then process the data where it is
 located.

Package: hadoop-native
Provides: hadoop-native
Architecture: i386 amd64
Depends: ${shlibs:Depends}, hadoop (= ${source:Version}), liblzo2-2, libzip1
Enhances: hadoop
Description: Native libraries for Hadoop (e.g., compression)
 This optional package contains native libraries that increase the performance
 of Hadoop's compression.

Package: hadoop-sbin
Provides: hadoop-sbin
Architecture: i386 amd64
Depends: ${shlibs:Depends}, hadoop (= ${source:Version})
Enhances: hadoop
Description: Server-side binaries necessary for secured Hadoop clusters
 This package contains a setuid program, 'task-controller', which is used for
 launching MapReduce tasks in a secured MapReduce cluster. This program allows
 the tasks to run as the Unix user who submitted the job, rather than the
 Unix user running the MapReduce daemons.
 .
 This package also contains 'jsvc', a daemon wrapper necessary to allow
 DataNodes to bind to a low (privileged) port and then drop root privileges
 before continuing operation.

Package: hadoop-fuse
Provides: hadoop-fuse
Architecture: i386 amd64
Depends: ${shlibs:Depends}, hadoop (= ${source:Version}), libfuse2, fuse-utils
Enhances: hadoop
Description: HDFS exposed over a Filesystem in Userspace
 These projects (enumerated below) allow HDFS to be mounted (on most flavors 
 of Unix) as a standard file system using the mount command. Once mounted, the
  user can operate on an instance of hdfs using standard Unix utilities such 
 as 'ls', 'cd', 'cp', 'mkdir', 'find', 'grep', or use standard Posix libraries 
 like open, write, read, close from C, C++, Python, Ruby, Perl, Java, bash, etc.

Package: hadoop-doc
Provides: hadoop-doc
Architecture: all
Section: doc
Description: Documentation for Hadoop
 This package contains the Java Documentation for Hadoop and its relevant
 APIs.

Package: hadoop-source
Provides: hadoop-source
Architecture: all
Description: Source code for Hadoop
 This package contains the source code for Hadoop and its contrib modules.

Package: hadoop-conf-pseudo
Provides: hadoop-conf-pseudo
Architecture: all
Depends: hadoop (= ${source:Version}), hadoop-namenode (= ${source:Version}), hadoop-datanode (= ${source:Version}), hadoop-secondarynamenode (= ${source:Version}), hadoop-jobtracker (= ${source:Version}), hadoop-tasktracker (= ${source:Version})
Description: Pseudo-distributed Hadoop configuration
 Contains configuration files for a "pseudo-distributed" Hadoop deployment.
 In this mode, each of the hadoop components runs as a separate Java process,
 but all on the same machine.

Package: hadoop-tasktracker
Provides: hadoop-tasktracker
Architecture: all
Depends: hadoop (= ${source:Version})
Description: Task Tracker for Hadoop
 The Task Tracker is the Hadoop service that accepts MapReduce tasks and
 computes results. Each node in a Hadoop cluster that should be doing
 computation should run a Task Tracker.

Package: hadoop-jobtracker
Provides: hadoop-jobtracker
Architecture: all
Depends: hadoop (= ${source:Version})
Description: Job Tracker for Hadoop
 The jobtracker is a central service which is responsible for managing
 the tasktracker services running on all nodes in a Hadoop Cluster.
 The jobtracker allocates work to the tasktracker nearest to the data
 with an available work slot.

Package: hadoop-namenode
Provides: hadoop-namenode
Architecture: all
Depends: hadoop (= ${source:Version})
Description: Name Node for Hadoop
 The Hadoop Distributed Filesystem (HDFS) requires one unique server, the
 namenode, which manages the block locations of files on the filesystem.

Package: hadoop-secondarynamenode
Provides: hadoop-secondarynamenode
Architecture: all
Depends: hadoop (= ${source:Version})
Description: Secondary Name Node for Hadoop
 The Secondary Name Node is responsible for checkpointing file system images.
 It is _not_ a failover pair for the namenode, and may safely be run on the
 same machine.

Package: hadoop-datanode
Provides: hadoop-datanode
Architecture: all
Depends: hadoop (= ${source:Version})
Description: Data Node for Hadoop
 The Data Nodes in the Hadoop Cluster are responsible for serving up
 blocks of data over the network to Hadoop Distributed Filesystem
 (HDFS) clients.

Package: hadoop-pipes
Provides: hadoop-pipes
Architecture: any
Depends: hadoop (= ${source:Version})
Description: Interface to author Hadoop MapReduce jobs in C++
 Contains Hadoop Pipes, a library which allows Hadoop MapReduce jobs to be
 written in C++.

Package: libhdfs0
Architecture: any
Depends: hadoop (= ${source:Version}), ${shlibs:Depends}
Description: JNI Bindings to access Hadoop HDFS from C
 See http://wiki.apache.org/hadoop/LibHDFS

Package: libhdfs0-dev
Architecture: any
Section: libdevel
Depends: hadoop (= ${source:Version}), libhdfs0 (= ${binary:Version})
Description: Development support for libhdfs0
 Includes examples and header files for accessing HDFS from C
